
#Regression Model

I've decided to analyze the data via a regression model. A regression model is
often used as a form of predictive analysis, however, that is not the goal here.
Obtaining a standardized regression model would help identify which predictors
(demographic information) are the most influential on the response variable (GPA).
All regression Models also have their 'R-squared' that tells you exactly how 
much the predictors really impact the responce.

#### Libraries Used
```{r}
library(caret)
library(lm.beta)

```

'white' variable has 'NA' as coefficient. This is likely because it is a linear
combination of other variables. (ie when all the other reaces in the model are
'0', then white is always '1'). 'White' is the first variable to be omitted 
from the model.


#### Creating Initial regression function with all predictors of interest
```{r}
reg <- lm(term_gpa ~  age + efc +
                         distance + studenttype2 +
                         American_Indian_or_Alaskan_Native + Asian + 
                         Black_African_American + Hispanic_Latino + 
                        International +
                         Native_Hawaiian_Pacific_Islander + Two_or_More +
                        White +
                         firstgen2 + sex2 + timestatus2 +
                         depstatus2 + pell2 +
                         child2,
                        data = ximp)
summary(reg)
```
'white' variable has 'NA' as coefficient. This is likely because it is a linear
combination of other variables. (ie when all the other reaces in the model are
'0', then white is always '1'). 'White' is the first variable to be omitted 
from the model.


#### Creates function that calculates VIF (variance inflation factor)
The VIF tells us if multicollinearity exists in the model. The goal here is to 
omit any predictors with VIF > 5. (high VIF = high multicollinearity)
```{r}
vif.lm <- function(object, ...) {
V <- summary(object)$cov.unscaled
Vi <- crossprod(model.matrix(object))
nam <- names(coef(object))
if(k <- match("(Intercept)", nam, nomatch = FALSE)) {
v1 <- diag(V)[-k]
v2 <- (diag(Vi)[-k] - Vi[k, -k]^2/Vi[k,k])
nam <- nam[-k]
} else {
v1 <- diag(V)
v2 <- diag(Vi)
warning("No intercept term detected. Results may surprise.")
}
structure(v1*v2, names = nam)
}
```


#### Create new model without 'White' Predictor and calculates VIF
```{r}
# remove white 
reg2 <- lm(term_gpa ~  + age + efc +
                         distance + studenttype2 +
                         American_Indian_or_Alaskan_Native + Asian + 
                         Black_African_American + Hispanic_Latino + 
                        International +
                         Native_Hawaiian_Pacific_Islander + Two_or_More +
                         firstgen2 + sex2 + timestatus2 +
                         depstatus2 + pell2 + child2,
                        data = ximp)
summary(reg2)
vif.lm(reg2)
```
VIF for all predictors are <5, none are omitted in this step.


#### Backward Stepwise Procedure
The backward stepwise procedure comes in handy when you have too many predictors
in your regression model and want to isolate the most important few. It drops
predictors one by one based on which one has highest p-value until all 
predictors have p-value <0.05.
```{r}
step(reg2, diectrion = 'backwards')
```



#### Regression Model that resulted from backward stepwise procedure
```{r}
reg3 <- lm(term_gpa ~ studenttype2 + International + sex2 + child2 + age + 
             American_Indian_or_Alaskan_Native + 
             Native_Hawaiian_Pacific_Islander + timestatus2 + efc +
             Black_African_American + Two_or_More + depstatus2 + distance +
             Hispanic_Latino + firstgen2 + pell2, ximp)
summary(reg3)
```
#### The varImp() ranks each predictor in terms of its effect on the responce.
higher vvalue = more important
```{r}
arrange(varImp(reg3), Overall)
```

```{r}
summary(lm.beta(reg3))

  
```



### What to derive from the regression model

I want to reitterate that this regression model would not serve as a good
predictive model. There are far too many predicotrs (risk overfitting) and the 
model has a low r-squared and high standard error. This is essentially 
because there are too many significant factors that exist outside our model 
that effect GPA. That being said, what can we take away from this model?

#### 1. Analyzing standard coefficients

Our model contains a mix of binary amd continous variables. 

You can interpret each binary variable coefficient as the average difference 
in GPA between the two possible values in the predictor.
EX: the coeeficient for 'sex2' is ~0.094. This means that women tend to have 
about 0.094 higher GPAs than men on average.

You can interpret coninuous variables as the average change in GPA for each one
unit increase in the predicotr. 
EX: the coeficient for 'age' is ~ 0.123. This means that a one year increase 
in age correlates with a 0.123 increase in GPA. A two year increase in age
correlates with a 0.246 (2 * 0.123) increase in GPA.

We can also look at the sign of each coefficient to see which factors tend to 
negatively impact GPA. Based on the sign of the coefficients, factors 
associated with lower GPA are:
- having one or more children
- Being black, hispanic, native american, alaskan native, 
  hawaiian/ pacific islander, or 2 or more races
- being a first-gen student (first in family to go to college)

#### 2. Looking at R-squared
The adjusted R-squared for the regression model is about 0.06. This means that 
of all the possible factors that effect GPA, only about 6% of them are included
in the model. In other words, a student's demographic profile accounts for
about 6% of all factors effecting their GPA. 



























